{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d80b9285",
   "metadata": {},
   "source": [
    "# Data Wrangling and EDA: Image Processing Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45f871c",
   "metadata": {},
   "source": [
    "# Overview of Image Classification and Recognition System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e38664",
   "metadata": {},
   "source": [
    "Image processing is used for everything from automatically tagging pictures in social media to unlocking our phones. Recent advancements in deep learning have made the accuracy of face recognition better. In this project, I will like to use the Humans dataset to develop a face classification/recognition system that can detect faces in images, identify the faces, and modify faces with digital makeup. I will start by setting up a development environment, and then train machine learning models to analyze images and identify facial landmarks. I will also code the facial feature detection, representing a face as a set of measurements, and encoding faces. Lastly, I will repurpose and adjust any of the existing ML systems I built.\n",
    "\n",
    "In this project, I will provide my ML system with examples of each image and then develop learning algorithms that look at these examples and learn about the visual appearance of each.\n",
    "\n",
    "That's why I created a a training dataset of labeled images, later I will feed them to the face recognition system to process the data. Overview of my system pipline will be as follows:\n",
    "\n",
    "Generate input which the training dataset with N images, each labeled based on race and gender.\n",
    "\n",
    "I use this training set to train a classifier/ML system to learn what every one of the images looks like.\n",
    "\n",
    "In the end, I will evaluate the quality of the classifier/ML system by asking it to predict images for a new set of images that it’s never seen before. I’ll then compare the true image to the ones predicted by the ML system.\n",
    "\n",
    "The dataset was taken from Kaggle(https://www.kaggle.com/ashwingupta3012/human-faces?select=Humans). It has a collection of 7.2k+ images. It has a mix of front face, side profile pictures which will help achieve great identifying results and an improved range of classifier/recognition possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30978f6",
   "metadata": {},
   "source": [
    "# Project Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d55219",
   "metadata": {},
   "source": [
    "Detecting faces in images.\n",
    "Analyzing a histogram of oriented gradients (HOG)\n",
    "Identifying faces in images.\n",
    "Locating facial features in images.\n",
    "Coding for face detection.\n",
    "Finding lookalikes using face detection.\n",
    "Generating face encoding automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12786816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, shutil, glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from skimage import color\n",
    "from skimage import io, filters, transform, restoration, measure, segmentation, feature\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import glob\n",
    "import skimage\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import face_recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae8677",
   "metadata": {},
   "source": [
    "# Loading Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a0906e",
   "metadata": {},
   "source": [
    "There are two folders with images. one is named train, that's where all the training images are stored. Humans_faces folder is where I will be picking random images to test the system later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b477d5",
   "metadata": {},
   "source": [
    "# Count number of images in the human_faces folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(dir):\n",
    "    return len([1 for x in list(os.scandir(dir)) if x.is_file()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35e097",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folder_hum_faces = '/Users/goldenrule/Desktop/Machine_Learning/Cap_3_file/Humans_faces'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a14f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_images(folder_hum_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92457be",
   "metadata": {},
   "source": [
    "there are 7211 images in the humans_faces folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be60408",
   "metadata": {},
   "source": [
    "# Count number of images in the train folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23ae48c",
   "metadata": {},
   "source": [
    "Creating variables for the folder containing the train file as folder and filename respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351cbb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/goldenrule/Desktop/Machine_Learning/Cap_3_file'\n",
    "filename_train = '/Users/goldenrule/Desktop/Machine_Learning/Cap_3_file/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea0933d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_images(filename_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b05af41",
   "metadata": {},
   "source": [
    "there are 7 images in the train folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c36b65",
   "metadata": {},
   "source": [
    "# Lets see what some of the training images look like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103f5c8b",
   "metadata": {},
   "source": [
    "Visualize the images in the train folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08115a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/goldenrule/Desktop/Machine_Learning/Cap_3_file/train/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfb751",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "io.use_plugin('matplotlib')\n",
    "collection = io.imread_collection(filename) #create a collection of all the images in the train folder\n",
    "plt.figure()\n",
    "io.imshow_collection(collection) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860573ef",
   "metadata": {},
   "source": [
    "I named a folder test, it has a duplicate of some of the images in the train folder in a different format. Let's see what they look like too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e769d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/goldenrule/Desktop/Machine_Learning/Cap_3_file/test/*'\n",
    "io.use_plugin('matplotlib')\n",
    "collection = io.imread_collection(filename) #create a collection of all the images in the train folder\n",
    "plt.figure()\n",
    "io.imshow_collection(collection) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e4f1ed",
   "metadata": {},
   "source": [
    "# Resized Images in the new images folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d581fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = '/Users/goldenrule/Desktop/Machine_Learning/Cap_3_file/new_images/*'\n",
    "io.use_plugin('matplotlib')\n",
    "collection = io.imread_collection(filename) #create a collection of all the images in the train folder\n",
    "plt.figure()\n",
    "io.imshow_collection(collection) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad2b326",
   "metadata": {},
   "source": [
    "# Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2580773",
   "metadata": {},
   "outputs": [],
   "source": [
    "I will apply the global thresholding to pick an area of the image I am interesting in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64817984",
   "metadata": {},
   "source": [
    "# Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd81545",
   "metadata": {},
   "source": [
    "I need to make sure the faces on the image files can be detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d47c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title='Image', cmap_type='gray'): \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    plt.imshow(image, cmap=cmap_type)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the white female image file into a numpy array\n",
    "image1 = face_recognition.load_image_file('train/train_wt_f.png')\n",
    "\n",
    "# Find all the faces in the image\n",
    "\n",
    "face_locations1 = face_recognition.face_locations(image1)\n",
    "number_of_faces = len(face_locations1)\n",
    "print(\"I found {} face(s) in this photograph.\".format(number_of_faces))\n",
    "\n",
    "# Load the image into a Python Image Library object so that we can draw on top of it and display it\n",
    "pil_image1 = PIL.Image.fromarray(image1)\n",
    "\n",
    "for face_location in face_locations1:\n",
    "\n",
    "    # Print the location of each face in this image. Each face is a list of co-ordinates in (top, right, bottom, left) order.\n",
    "    top, right, bottom, left = face_location\n",
    "    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n",
    "\n",
    "    # Let's draw a box around the face\n",
    "    draw = PIL.ImageDraw.Draw(pil_image)\n",
    "    draw.rectangle([left, top, right, bottom], outline=\"red\")\n",
    "\n",
    "# Display the image on screen\n",
    "show_image(pil_image1, 'train_white_female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b08e36a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the old white male file into a numpy array\n",
    "image2 = face_recognition.load_image_file('train/train_old_wt_m.jpeg')\n",
    "\n",
    "# Find all the faces in the image\n",
    "\n",
    "face_locations2 = face_recognition.face_locations(image2)\n",
    "number_of_faces = len(face_locations2)\n",
    "print(\"I found {} face(s) in this photograph.\".format(number_of_faces))\n",
    "\n",
    "# Load the image into a Python Image Library object so that we can draw on top of it and display it\n",
    "pil_image2 = PIL.Image.fromarray(image2)\n",
    "\n",
    "for face_location in face_locations2:\n",
    "\n",
    "    # Print the location of each face in this image. Each face is a list of co-ordinates in (top, right, bottom, left) order.\n",
    "    top, right, bottom, left = face_location\n",
    "    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n",
    "\n",
    "    # Let's draw a box around the face\n",
    "    draw = PIL.ImageDraw.Draw(pil_image2)\n",
    "    draw.rectangle([left, top, right, bottom], outline=\"red\")\n",
    "\n",
    "# Display the image on screen\n",
    "show_image(pil_image2, 'traing_old_white_male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb80f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the black male image file into a numpy array\n",
    "image3 = face_recognition.load_image_file('train/train_bk_m.jpg')\n",
    "\n",
    "# Find all the faces in the image\n",
    "\n",
    "face_locations3 = face_recognition.face_locations(image3)\n",
    "number_of_faces = len(face_locations3)\n",
    "print(\"I found {} face(s) in this photograph.\".format(number_of_faces))\n",
    "\n",
    "# Load the image into a Python Image Library object so that we can draw on top of it and display it\n",
    "pil_image3 = PIL.Image.fromarray(image3)\n",
    "\n",
    "for face_location in face_locations3:\n",
    "\n",
    "    # Print the location of each face in this image. Each face is a list of co-ordinates in (top, right, bottom, left) order.\n",
    "    top, right, bottom, left = face_location\n",
    "    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n",
    "\n",
    "    # Let's draw a box around the face\n",
    "    draw = PIL.ImageDraw.Draw(pil_image3)\n",
    "    draw.rectangle([left, top, right, bottom], outline=\"red\")\n",
    "\n",
    "# Display the image on screen\n",
    "show_image(pil_image3, 'train_black_male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bf96e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the second white female drawing image file into a numpy array\n",
    "image4 = face_recognition.load_image_file('train/Train_wt_f2.jpg')\n",
    "\n",
    "# Find all the faces in the image\n",
    "\n",
    "face_locations4 = face_recognition.face_locations(image4)\n",
    "number_of_faces = len(face_locations4)\n",
    "print(\"I found {} face(s) in this photograph.\".format(number_of_faces))\n",
    "\n",
    "# Load the image into a Python Image Library object so that we can draw on top of it and display it\n",
    "pil_image4 = PIL.Image.fromarray(image4)\n",
    "\n",
    "for face_location in face_locations4:\n",
    "\n",
    "    # Print the location of each face in this image. Each face is a list of co-ordinates in (top, right, bottom, left) order.\n",
    "    top, right, bottom, left = face_location\n",
    "    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n",
    "\n",
    "    # Let's draw a box around the face\n",
    "    draw = PIL.ImageDraw.Draw(pil_image4)\n",
    "    draw.rectangle([left, top, right, bottom], outline=\"red\")\n",
    "\n",
    "# Display the image on screen\n",
    "show_image(pil_image4, 'train_white_female_drawing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf3f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the second white female drawing image file into a numpy array\n",
    "image5 = face_recognition.load_image_file('train/train_wt_m.jpg')\n",
    "\n",
    "# Find all the faces in the image\n",
    "\n",
    "face_locations5 = face_recognition.face_locations(image5)\n",
    "number_of_faces = len(face_locations5)\n",
    "print(\"I found {} face(s) in this photograph.\".format(number_of_faces))\n",
    "\n",
    "# Load the image into a Python Image Library object so that we can draw on top of it and display it\n",
    "pil_image5 = PIL.Image.fromarray(image5)\n",
    "\n",
    "for face_location in face_locations5:\n",
    "\n",
    "    # Print the location of each face in this image. Each face is a list of co-ordinates in (top, right, bottom, left) order.\n",
    "    top, right, bottom, left = face_location\n",
    "    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n",
    "\n",
    "    # Let's draw a box around the face\n",
    "    draw = PIL.ImageDraw.Draw(pil_image5)\n",
    "    draw.rectangle([left, top, right, bottom], outline=\"red\")\n",
    "\n",
    "# Display the image on screen\n",
    "show_image(pil_image5, 'train_white_male')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c277b",
   "metadata": {},
   "source": [
    "I want to test the alogrithims on an image with two people to see if it can detect the two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d979950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the second white female drawing image file into a numpy array\n",
    "image6 = face_recognition.load_image_file('test/testing_f.jpg')\n",
    "\n",
    "# Find all the faces in the image\n",
    "\n",
    "face_locations6 = face_recognition.face_locations(image6)\n",
    "number_of_faces = len(face_locations6)\n",
    "print(\"I found {} face(s) in this photograph.\".format(number_of_faces))\n",
    "\n",
    "# Load the image into a Python Image Library object so that we can draw on top of it and display it\n",
    "pil_image6 = PIL.Image.fromarray(image6)\n",
    "\n",
    "for face_location in face_locations6:\n",
    "\n",
    "    # Print the location of each face in this image. Each face is a list of co-ordinates in (top, right, bottom, left) order.\n",
    "    top, right, bottom, left = face_location\n",
    "    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n",
    "\n",
    "    # Let's draw a box around the face\n",
    "    draw = PIL.ImageDraw.Draw(pil_image6)\n",
    "    draw.rectangle([left, top, right, bottom], outline=\"red\")\n",
    "\n",
    "# Display the image on screen\n",
    "show_image(pil_image6, 'train_white_male')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e451d0",
   "metadata": {},
   "source": [
    "I think it works great!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674de7a",
   "metadata": {},
   "source": [
    "# Face Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987dd9e1",
   "metadata": {},
   "source": [
    "Since the algorithm only sees an array of RGB values that matches a pattern from the sample images. It notes information like color, size, gaps, etc and use it to identify a particular face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c71f8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate the face encodings\n",
    "face_encoding6 = face_recognition.face_encodings(image6) #face enconding for the testing image file\n",
    "\n",
    "if len(face_encoding6) == 0:\n",
    "    # No faces found in the image.\n",
    "    print(\"No faces were found.\")\n",
    "\n",
    "else:\n",
    "    # Grab the first face encoding\n",
    "    first_face_encoding = face_encoding6[0]\n",
    "\n",
    "    # Print the results\n",
    "    print(first_face_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a9743",
   "metadata": {},
   "source": [
    "Face enconding for the rest of the training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14430484",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_encoding3 = face_recognition.face_encodings(image3, known_face_locations=face_locations3)[0]\n",
    "face_encoding5 = face_recognition.face_encodings(image5, known_face_locations=face_locations5)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bb97a",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f2aba",
   "metadata": {},
   "source": [
    "I want to use the face enconding of the white female drawing and white female image to see if there are similiarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#face enconding for the white female image\n",
    "face_encoding1 = face_recognition.face_encodings(image1, known_face_locations=face_locations1)\n",
    "#face enconding for the white female drawing\n",
    "face_encoding4 = face_recognition.face_encodings(image4, known_face_locations=face_locations4)[0]\n",
    "# Compare the two images to see if there are similar or not\n",
    "face_recognition.compare_faces(face_encoding1, face_encoding4, tolerance=0.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d7f4a0",
   "metadata": {},
   "source": [
    "I will intentionally compare the two images for the old white male. One is in the test folder while the order is in the train folder. There are both in a different file formart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdd90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the the image in the test folder as image 7\n",
    "image7 = face_recognition.load_image_file('test/test_old_wt_m.jpeg')\n",
    "\n",
    "# Find all the faces in the image\n",
    "face_locations7 = face_recognition.face_locations(image7)\n",
    "\n",
    "#face enconding for the white old male in the train folder\n",
    "face_encoding2 = face_recognition.face_encodings(image2, known_face_locations=face_locations2)[0]\n",
    "#face enconding for the white old male in the test folder\n",
    "face_encoding7 = face_recognition.face_encodings(image7, known_face_locations=face_locations7)\n",
    "# Compare the two images to see if there are similar or not\n",
    "face_recognition.compare_faces(face_encoding2, face_encoding7, tolerance=0.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2cb90",
   "metadata": {},
   "source": [
    "I see that the comparison is correct, it returned true for both images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a594f95",
   "metadata": {},
   "source": [
    "# Face Landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3361a7",
   "metadata": {},
   "source": [
    "I want to use the face_landmarks library to identify individual features of the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ce52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the face landmark function\n",
    "face_landmarks_list = face_recognition.face_landmarks(image1) #using image1, the image of the white female\n",
    "# Loop over each face\n",
    "for face_landmarks in face_landmarks_list:\n",
    "\n",
    "    # Loop over each facial feature (eye, nose, mouth, lips, etc)\n",
    "    for name, list_of_points in face_landmarks.items():\n",
    "\n",
    "        # Print the location of each facial feature in this image\n",
    "        print(\"The {} in this face has the following points: {}\".format(name, list_of_points))\n",
    "\n",
    "        # Let's trace out each facial feature in the image with a line!\n",
    "        draw.line(list_of_points, fill=\"red\", width=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bceba68",
   "metadata": {},
   "source": [
    "# Turning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2cbc11",
   "metadata": {},
   "source": [
    "This random function will pick images randomly from the Human folder and I will use it to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecd4a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_hum_faces = '/Users/goldenrule/Desktop/Machine_Learning/Cap_3_file/Humans_faces'\n",
    "a=random.choice(os.listdir(folder_hum_faces))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4af52d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all known face encodings\n",
    "known_face_encodings = [\n",
    "    face_encoding1,\n",
    "    face_encoding2,\n",
    "    face_encoding4,\n",
    "    face_encoding5\n",
    "][0]\n",
    "\n",
    "# Load the image we want to check\n",
    "unknown_image = face_recognition.load_image_file('train/train_wt_m.jpg')\n",
    "\n",
    "# Get face encodings for any people in the picture\n",
    "face_locations = face_recognition.face_locations(unknown_image)\n",
    "unknown_face_encodings = face_recognition.face_encodings(unknown_image, known_face_locations = face_locations)\n",
    "\n",
    "# There might be more than one person in the photo, so we need to loop over each face we found\n",
    "for filename in os.listdir(folder_hum_faces):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\") or filename.endswith(\".JPG\"):\n",
    "        print(filename)\n",
    "        f = os.path.join(folder_hum_faces, filename)\n",
    "\n",
    "        unknown_image = face_recognition.load_image_file(f)\n",
    "        #unknown_encoding = face_recognition.face_encodings(unknown_image)\n",
    "        if not len(unknown_face_encodings):\n",
    "            print(filename, \"can't be encoded\")\n",
    "            continue\n",
    "\n",
    "        results = face_recognition.compare_faces(known_face_encodings, unknown_face_encodings[0])\n",
    "        name = \"Unknown\"\n",
    "        if results[0]:\n",
    "            name = \"Person 1\"\n",
    "        elif results[1]:\n",
    "            name = \"Person 2\"\n",
    "        elif results[2]:\n",
    "            name = \"Person 3\"\n",
    "        print(f\"Found {name} in the photo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe789b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all known face encodings\n",
    "known_face_encodings = [\n",
    "    face_encoding1,\n",
    "    face_encoding2,\n",
    "    face_encoding3,\n",
    "    face_encoding4,\n",
    "    face_encoding5,\n",
    "    face_encoding6\n",
    "][0]\n",
    "\n",
    "# Load the image we want to check\n",
    "unknown_image = face_recognition.load_image_file('test/test_old_wt_m.jpeg')\n",
    "\n",
    "# Get face encodings for any people in the picture\n",
    "face_locations = face_recognition.face_locations(unknown_image)\n",
    "unknown_face_encodings = face_recognition.face_encodings(unknown_image)\n",
    "\n",
    "# There might be more than one person in the photo, so we need to loop over each face we found\n",
    "for unknown_face_encoding in unknown_face_encodings:\n",
    "\n",
    "    # Test if this unknown face encoding matches any of the three people we know\n",
    "    results = face_recognition.compare_faces(known_face_encodings, unknown_face_encoding)\n",
    "    name = \"Unknown\"\n",
    "\n",
    "    if results[0]:\n",
    "        name = \"Person 1\"\n",
    "    elif results[1]:\n",
    "        name = \"Person 2\"\n",
    "    elif results[2]:\n",
    "        name = \"Person 3\"\n",
    "\n",
    "    print(f\"Found {name} in the photo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24812d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(original, new_image, title_resized):\n",
    "    plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    fig, (ax1, ax2) = \\\n",
    "        plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "    ax1.imshow(original, cmap=plt.cm.gray)\n",
    "    ax1.set_title('original')\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(new_image, cmap=plt.cm.gray)\n",
    "    ax2.set_title(title_resized)\n",
    "    ax2.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772174ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(image1, unknown_image, \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b17e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(image1, \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb5d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the the image in the test folder as image 7\n",
    "image8 = face_recognition.load_image_file('Humans_faces/1 (2763).jpg')\n",
    "\n",
    "# Find all the faces in the image\n",
    "face_locations8 = face_recognition.face_locations(image8)\n",
    "\n",
    "#face enconding for the white old male in the train folder\n",
    "#face_encoding2 = face_recognition.face_encodings(image2, known_face_locations=face_locations)[0]\n",
    "#face enconding for the white old male in the test folder\n",
    "face_encoding8 = face_recognition.face_encodings(image8, known_face_locations=face_locations8)[0]\n",
    "# Compare the two images to see if there are similar or not\n",
    "face_recognition.compare_faces(face_encoding1, face_encoding8, tolerance=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655de759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3523003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yen_thresh = filters.threshold_yen(grayscale_white_f)\n",
    "binary_white_f = 255 * (grayscale_white_f > yen_thresh)\n",
    "show_image(binary_white_f, 'white female thresholded with Li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fd766",
   "metadata": {},
   "outputs": [],
   "source": [
    "I think yen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33eb84b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "white_f = io.imread('new_images/new_t_wtf400.jpeg')\n",
    "grayscale_white_f = color.rgb2gray(white_f)\n",
    "fig, ax = filters.try_all_threshold(grayscale_white_f, verbose=False)\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
